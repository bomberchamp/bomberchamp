{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import s, e\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import HTML, clear_output, display, update_display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from simple import Game, get_valid_actions\n",
    "\n",
    "from agent_code.simple_agent.wrapper import SimpleAgent\n",
    "\n",
    "from agent_code.tensor_agent.agent import TensorAgent\n",
    "from agent_code.tensor_agent.hyperparameters import hp\n",
    "from agent_code.tensor_agent.X import RelativeX3 as game_state_X\n",
    "from agent_code.tensor_agent.X import AbsoluteX3, X3_to_imgs\n",
    "from agent_code.tensor_agent.model import FullModel, Counter\n",
    "\n",
    "choices = ['RIGHT', 'LEFT', 'UP', 'DOWN', 'BOMB', 'WAIT']\n",
    "action_y_map = {choices[i]: i for i in range(len(choices))}\n",
    "D = len(choices)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.buffer_size = 100000\n",
    "hp.target_network_period = 32000\n",
    "hp.epsilon = 0.\n",
    "hp.learning_rate=0.000625\n",
    "hp.discount_factor=0.99\n",
    "hp.peaceful = False\n",
    "hp.multi_step_n = 1\n",
    "\n",
    "aux_rewards = {\n",
    "    e.WAITED: -0.2,\n",
    "    e.CRATE_DESTROYED: 0.2,\n",
    "    e.COIN_COLLECTED: 0,\n",
    "    e.KILLED_OPPONENT: 0,\n",
    "    e.KILLED_SELF: 0,\n",
    "    e.GOT_KILLED: -5\n",
    "}\n",
    "\n",
    "last_moves_length = 20\n",
    "last_moves_similarity_penalty = 1 / last_moves_length\n",
    "\n",
    "hurry_up = 1 * (1 - hp.discount_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallOfFame:\n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "    \n",
    "    def add(self, weights):\n",
    "        self.weights.append(weights)\n",
    "        if len(self.weights) > 50:\n",
    "            self.weights = self.weights[::2]\n",
    "\n",
    "HoF = HallOfFame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "K.clear_session()\n",
    "#model = FullModel(game_state_X.shape, D)\n",
    "\n",
    "def make_agent():\n",
    "    return TensorAgent(game_state_X.shape, D, weights=None, model=FullModel(game_state_X.shape, D))\n",
    "\n",
    "total_step = 0\n",
    "\n",
    "tensor_agent = make_agent()\n",
    "\n",
    "original = 'tensor_agent'\n",
    "copies = [f'tensor_agent-copy{i}' for i in range(3)]\n",
    "additional = [f'tensor_agent{i}' for i in range(3)]\n",
    "simples = [f'simple_agent{i}' for i in range(3)]\n",
    "\n",
    "agents = {\n",
    "    original: tensor_agent\n",
    "}\n",
    "\n",
    "for n in copies:\n",
    "    agents[n] = tensor_agent.clone()\n",
    "\n",
    "for n in additional:\n",
    "    agents[n] = make_agent()\n",
    "\n",
    "for n in simples:\n",
    "    agents[n] = SimpleAgent(game_state_X.shape, D, model=tensor_agent.model, er_buffer=tensor_agent.buffer)\n",
    "\n",
    "train = {a: False for a in agents}\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights\n",
      "weights loaded\n"
     ]
    }
   ],
   "source": [
    "tensor_agent.model.load_weights('models/self-play10/tensor_agent-model4000000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights\n",
      "weights loaded\n"
     ]
    }
   ],
   "source": [
    "agents[additional[0]].model.load_weights('self-play2-600k.h5')\n",
    "HoF.add(agents[additional[0]].model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step=4000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_code.tensor_agent.layers import NoisyDense\n",
    "\n",
    "for layer in tensor_agent.model.online.layers + tensor_agent.model.target.layers:\n",
    "    if type(layer) == NoisyDense:\n",
    "        layer.w_sigma.initializer.run(session=K.get_session())\n",
    "        layer.b_sigma.initializer.run(session=K.get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentInfo(object):\n",
    "    def __init__(self, agents, moving_rewards):\n",
    "        self.agents = agents\n",
    "        self.moving_rewards = moving_rewards\n",
    "\n",
    "    def _repr_pretty_(self, pp, cycle):\n",
    "        text = ''\n",
    "        for n, a in self.agents.items():\n",
    "            text +=  '=====================\\n'\n",
    "            text += f'{n} ({a.model.family}) \\n'\n",
    "            text += f'trained: {a.model.steps} \\n'\n",
    "            text += f'moving reward: {moving_rewards[n]:.2f} \\n'\n",
    "        pp.text(text)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.agents.keys().join(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 33, 33, 6)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 33, 33, 32)   224         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 17, 17, 64)   32832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 9, 9, 64)     36928       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 13, 13, 6)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 4, 4, 64)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 5, 5, 6)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 13, 13, 64)   3520        cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 4, 4, 64)     36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 5, 5, 64)     3520        cropping2d[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1024)         0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1600)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3136)         0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5760)         0           flatten_2[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense (NoisyDense)        (None, 512)          5899264     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_2 (NoisyDense)      (None, 512)          5899264     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_1 (NoisyDense)      (None, 1)            1026        noisy_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "noisy_dense_3 (NoisyDense)      (None, 6)            6156        noisy_dense_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "va_merge (VAMerge)              (None, 6)            0           noisy_dense_1[0][0]              \n",
      "                                                                 noisy_dense_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 11,956,590\n",
      "Trainable params: 11,956,590\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tensor_agent.model.online.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Complete'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "=====================\n",
       "tensor_agent (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 2.19 \n",
       "=====================\n",
       "tensor_agent-copy0 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 4.57 \n",
       "=====================\n",
       "tensor_agent-copy1 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 2.24 \n",
       "=====================\n",
       "tensor_agent-copy2 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 1.10 \n",
       "=====================\n",
       "tensor_agent0 (zealous_coin_researcher) \n",
       "trained: 0 \n",
       "moving reward: 0.65 \n",
       "=====================\n",
       "tensor_agent1 (stoic_crate_researcher) \n",
       "trained: 0 \n",
       "moving reward: 3.40 \n",
       "=====================\n",
       "tensor_agent2 (hardcore_coin_advocate) \n",
       "trained: 0 \n",
       "moving reward: 4.49 \n",
       "=====================\n",
       "simple_agent0 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 1.04 \n",
       "=====================\n",
       "simple_agent1 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 0.50 \n",
       "=====================\n",
       "simple_agent2 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 1.48 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = display(f'Starting...', display_id='progress')\n",
    "\n",
    "\n",
    "n_steps = 5000000\n",
    "game = None\n",
    "current_agents = {}\n",
    "\n",
    "train = {n: False for n in agents.keys()}\n",
    "for n in [original] + copies + simples:\n",
    "    train[n] = True\n",
    "#train['tensor_agent'] = True\n",
    "\n",
    "moving_rewards = {n: 0 for n in agents.keys()}\n",
    "episode_count = 0\n",
    "\n",
    "info = AgentInfo(agents, moving_rewards)\n",
    "agent_display = display(info, display_id='agent_info')\n",
    "\n",
    "\n",
    "\n",
    "for step in range(total_step, n_steps):\n",
    "    if game is None:\n",
    "        episode_count += 1\n",
    "        \n",
    "        #=== Choose Agents ===\n",
    "        agent_choice = np.random.choice([1, 2, 3, 4], p=[0.5, 0., 0., 0.5])\n",
    "        if agent_choice == 1:\n",
    "            agent_choice = 'solo'\n",
    "            current_agents = {n: agents[n] for n in [original]}\n",
    "        else:\n",
    "            agent_choices = np.random.choice(['hof', 'clones', 'simples'], agent_choice-1, p=[0.3, 0.4, 0.3])\n",
    "            current_agents = {original: agents[original]}\n",
    "            \n",
    "            for key, cnt in zip(*np.unique(agent_choices, return_counts=True)):\n",
    "                for i in range(cnt):\n",
    "                    if key == 'hof':\n",
    "                        if len(HoF.weights) > 2:\n",
    "                            n = additional[i]\n",
    "                            agents[n].model.set_weights(HoF.weights[np.random.choice(len(HoF.weights))])\n",
    "                        else:\n",
    "                            n = copies[i]\n",
    "                    elif key == 'clones':\n",
    "                        n = copies[i]\n",
    "                    elif key == 'simples':\n",
    "                        n = simples[i]\n",
    "            \n",
    "                    current_agents[n] = agents[n]\n",
    "        \n",
    "        last_moves = {n: deque(maxlen=last_moves_length) for n in current_agents.keys()}\n",
    "        \n",
    "        \n",
    "        #=== Choose Game ===\n",
    "        choice = np.random.choice(['coins', 'deathmatch', 'full'], p=[0.5,0.,0.5])\n",
    "        if choice == 'coins':\n",
    "            game = Game(*Game.create_arena(current_agents.keys(), crate_density=0.), \\\n",
    "                        max_duration=np.random.randint(100, 201))\n",
    "        elif choice == 'deathmatch' and agent_choice != 'solo':\n",
    "            game = Game(*Game.create_arena(current_agents.keys(), crate_density=0., coins_per_area=0),\\\n",
    "                        max_duration=np.random.randint(100, 401))\n",
    "        else: # 'full'\n",
    "            game = Game(*Game.create_arena(current_agents.keys(),\\\n",
    "                                           crate_density=np.random.uniform(low=0.5, high=1.0)), \\\n",
    "                       max_duration=np.random.randint(200, 401))\n",
    "        dead_rewards = {}\n",
    "\n",
    "    total_step += 1\n",
    "    actions = {}\n",
    "    Xs = {}\n",
    "    for agent in game.agents:\n",
    "        x, y, name, b, _ = agent\n",
    "        \n",
    "        game_state = game.get_game_state(agent)\n",
    "        Xs[name] = game_state_X.get(game_state)\n",
    "        valid_actions = get_valid_actions(x, y, b, game, prevent_death=False)\n",
    "        actions[name] = current_agents[name].act(Xs[name], train=train[name], valid_actions=valid_actions, game_state=game_state)\n",
    "    \n",
    "    actions_as_string = {n: choices[actions[n]] for n in actions.keys()}\n",
    "    rewards, events = game.step(actions_as_string)\n",
    "    \n",
    "    ex_rewards = {}\n",
    "    for name in actions.keys():\n",
    "        moving_rewards[name] = 0.99 * moving_rewards[name] + rewards[name]\n",
    "        ex_rewards[name] = rewards[name] + np.sum([events[name][event] * aux_rewards[event] for event in aux_rewards.keys()])\n",
    "    \n",
    "    #=== Delayed reward on death ===\n",
    "    for name in actions.keys():\n",
    "        if events[name][e.GOT_KILLED]:\n",
    "            dead_rewards[name] = [ex_rewards[name], np.maximum(len(game.agents)-1, 1), Xs[name], actions[name]]\n",
    "    \n",
    "    for _,_,name,_,_ in game.agents:\n",
    "        for dr in dead_rewards.values():\n",
    "            dr[0] -= rewards[name]\n",
    "    #===============================\n",
    "    \n",
    "    for x,y,name,_,_ in game.agents:\n",
    "        reward = ex_rewards[name]\n",
    "        \n",
    "        reward -= last_moves[name].count((x, y, actions[name])) * last_moves_similarity_penalty\n",
    "        last_moves[name].append((x, y, actions[name]))\n",
    "        \n",
    "        for _,_,opponent,_,_ in game.agents:\n",
    "            if opponent != name:\n",
    "                reward -= rewards[opponent] / (len(game.agents) - 1)\n",
    "        \n",
    "        if game.terminated:\n",
    "            reward -= np.sum(game.coins) / len(game.agents)\n",
    "        \n",
    "        if train[name]:\n",
    "            current_agents[name].reward_update([Xs[name], actions[name], reward])\n",
    "            \n",
    "    \n",
    "    agent_display.update(info)\n",
    "    \n",
    "    if step % 100000 == 0:\n",
    "        tensor_agent.model.save(f'models/self-play10/tensor_agent-model{total_step}.h5')\n",
    "    \n",
    "    if game.terminated:\n",
    "        # Apply delayed reward for death\n",
    "        for name, dr in dead_rewards.items():\n",
    "            if train[name]:\n",
    "                current_agents[name].reward_update([dr[2], dr[3], aux_rewards[e.GOT_KILLED]])#(dr[0]-np.sum(game.coins))/dr[1]])\n",
    "        \n",
    "        for name, a in current_agents.items():\n",
    "            a.end_of_episode() # alt: save=None\n",
    "        \n",
    "        if episode_count % 100 == 0:\n",
    "            HoF.add(tensor_agent.model.get_weights())\n",
    "        \n",
    "        d.update(f'Episode {episode_count} Step: {step+1}/{n_steps}')\n",
    "        game = None\n",
    "\n",
    "d.update(f'Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_agent.model.save(f'models/self-play10/tensor_agent-model{total_step}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights\n",
      "weights loaded\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step: 250/1000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "=====================\n",
       "tensor_agent (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 1.73 \n",
       "=====================\n",
       "simple_agent0 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 1.00 \n",
       "=====================\n",
       "simple_agent1 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 0.61 \n",
       "=====================\n",
       "simple_agent2 (zealous_crate_researcher) \n",
       "trained: 371588 \n",
       "moving reward: 1.80 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor_agent': 'WAIT', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'WAIT', 'simple_agent0': 'BOMB', 'simple_agent1': 'BOMB', 'simple_agent2': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT', 'simple_agent1': 'LEFT', 'simple_agent2': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN', 'simple_agent2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN', 'simple_agent2': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP', 'simple_agent1': 'UP', 'simple_agent2': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN', 'simple_agent2': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'BOMB', 'simple_agent1': 'BOMB', 'simple_agent2': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP', 'simple_agent1': 'UP', 'simple_agent2': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP', 'simple_agent1': 'UP', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'BOMB', 'simple_agent1': 'BOMB', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT', 'simple_agent1': 'LEFT', 'simple_agent2': 'BOMB'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'RIGHT', 'simple_agent1': 'LEFT', 'simple_agent2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT', 'simple_agent1': 'DOWN', 'simple_agent2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT', 'simple_agent1': 'DOWN', 'simple_agent2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN', 'simple_agent1': 'RIGHT', 'simple_agent2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN', 'simple_agent1': 'LEFT', 'simple_agent2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN', 'simple_agent1': 'RIGHT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT', 'simple_agent1': 'DOWN', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'BOMB', 'simple_agent1': 'BOMB', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT', 'simple_agent1': 'UP', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN', 'simple_agent1': 'LEFT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN', 'simple_agent1': 'LEFT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP', 'simple_agent1': 'LEFT', 'simple_agent2': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP', 'simple_agent1': 'LEFT', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP', 'simple_agent1': 'DOWN', 'simple_agent2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT', 'simple_agent1': 'BOMB'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT', 'simple_agent1': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP', 'simple_agent1': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP', 'simple_agent1': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP', 'simple_agent1': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP', 'simple_agent1': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT', 'simple_agent1': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT', 'simple_agent1': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN', 'simple_agent1': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'BOMB', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT', 'simple_agent1': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN', 'simple_agent1': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'WAIT', 'simple_agent0': 'LEFT', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN', 'simple_agent1': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'UP', 'simple_agent1': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN', 'simple_agent1': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'WAIT', 'simple_agent0': 'BOMB'}\n",
      "{'tensor_agent': 'WAIT', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'simple_agent0': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'simple_agent0': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'simple_agent0': 'RIGHT'}\n",
      "{'simple_agent0': 'LEFT'}\n",
      "{'simple_agent0': 'BOMB'}\n",
      "{'simple_agent0': 'RIGHT'}\n",
      "{'simple_agent0': 'RIGHT'}\n",
      "{'simple_agent0': 'DOWN'}\n",
      "{'simple_agent0': 'DOWN'}\n",
      "{'simple_agent0': 'LEFT'}\n"
     ]
    }
   ],
   "source": [
    "d = display(f'Starting...', display_id='progress_test')\n",
    "\n",
    "n_steps = 1000\n",
    "game = None\n",
    "\n",
    "current_agents = {n: agents[n] for n in [original] + simples}\n",
    "train = {n: False for n in agents.keys()}\n",
    "moving_rewards = {n: 0 for n in current_agents.keys()}\n",
    "\n",
    "info = AgentInfo(current_agents, moving_rewards)\n",
    "agent_display = display(info, display_id='agent_info_test')\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    if game is None:\n",
    "        game = Game(*Game.create_arena(current_agents.keys(), crate_density=0.75))\n",
    "\n",
    "    actions = {}\n",
    "    Xs = {}\n",
    "    img = np.zeros((17, 17, 4))\n",
    "    \n",
    "    for agent in game.agents:\n",
    "        x, y, name, b, _ = agent\n",
    "        \n",
    "        game_state = game.get_game_state(agent)\n",
    "        #img[:,:,0] = (game_state['arena'] == -1) * 0.75\n",
    "        #img[:,:,0] += game_state['arena'] == 1\n",
    "        #coins = game_state['coins']\n",
    "        #for i in range(len(coins)):\n",
    "        #    img[coins[i][0], coins[i][1], 1] = 0.75\n",
    "        #img[x,y,1] = 1\n",
    "        #img[:,:,2] = game_state['explosions'] / np.max(game_state['explosions'])\n",
    "        #bombs = game_state['bombs']\n",
    "        #\n",
    "        #for i in range(len(bombs)):\n",
    "        #    img[bombs[i][0], bombs[i][1], 2] = 0.75 - bombs[i][2] / (s.bomb_timer) / 2\n",
    "        \n",
    "        Xs[name] = game_state_X.get(game_state)\n",
    "        _, img = X3_to_imgs(AbsoluteX3.get(game_state), whitening=10)\n",
    "        \n",
    "        valid_actions = get_valid_actions(x, y, b, game)\n",
    "        actions[name] = current_agents[name].act(Xs[name], train=train[name], valid_actions=valid_actions, game_state=game_state)\n",
    "    \n",
    "    imgs.append(img)\n",
    "    \n",
    "    actions_as_string = {n: choices[actions[n]] for n in actions.keys()}\n",
    "    print(actions_as_string)\n",
    "    rewards, events = game.step(actions_as_string)\n",
    "    \n",
    "    for name in actions.keys():\n",
    "        moving_rewards[name] = 0.99 * moving_rewards[name] + rewards[name]\n",
    "    \n",
    "    d.update(f'Step: {step+1}/{n_steps}')\n",
    "    agent_display.update(info)\n",
    "    \n",
    "    if game.terminated:\n",
    "        #for name, a in agents.items():\n",
    "            #a.end_of_episode(save='tensor_agent-model.h5') # alt: save=None\n",
    "        game_state = game.get_game_state(None)\n",
    "        _, img = X3_to_imgs(AbsoluteX3.get(game_state))\n",
    "        imgs.append(img)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animation(imgs, interval=50):\n",
    "    import matplotlib.animation\n",
    "    \n",
    "    steps = len(imgs)\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(t):\n",
    "        plt.imshow(imgs[t])\n",
    "\n",
    "    ani = matplotlib.animation.FuncAnimation(fig, animate, frames=steps, interval=interval)\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = animation(imgs[:60], interval=300)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(slices[22])):\n",
    "#    slices[22][i].save(f'tex/images/X_channel-{i}-raw.png', 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm anim/*\n",
    "for i in range(len(imgs)):\n",
    "    Image.fromarray(np.uint8(imgs[i]*255)).resize((17*10,17*10)).save('anim/{:0>3d}.png'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "! convert anim/*.png movie.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get update\n",
    "! apt-get install -y imagemagick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensor_agent': 8, 'simple_agent0': 8, 'simple_agent1': 1, 'simple_agent2': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
