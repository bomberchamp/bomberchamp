{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import s, e\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle\n",
    "\n",
    "from IPython.display import HTML, clear_output, display, update_display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from simple import Game, get_valid_actions\n",
    "\n",
    "from agent_code.tensor_agent.agent import TensorAgent\n",
    "from agent_code.tensor_agent.hyperparameters import hp\n",
    "from agent_code.tensor_agent.X import RelativeX2 as game_state_X\n",
    "from agent_code.tensor_agent.model import FullModel, Counter\n",
    "\n",
    "choices = ['RIGHT', 'LEFT', 'UP', 'DOWN', 'BOMB', 'WAIT']\n",
    "action_y_map = {choices[i]: i for i in range(len(choices))}\n",
    "D = len(choices)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.buffer_size = 100000\n",
    "hp.target_network_period = 32000\n",
    "hp.epsilon = 0.\n",
    "hp.learning_rate=0.0001\n",
    "hp.discount_factor=0.99\n",
    "hp.peaceful = False\n",
    "\n",
    "aux_rewards = {\n",
    "    e.WAITED: -0.1,\n",
    "    e.CRATE_DESTROYED: 0.2,\n",
    "    e.COIN_COLLECTED: 0,\n",
    "    e.KILLED_OPPONENT: 0,\n",
    "    e.KILLED_SELF: 0,\n",
    "    e.GOT_KILLED: 0\n",
    "}\n",
    "\n",
    "hurry_up = 1 * (1 - hp.discount_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HallOfFame:\n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "    \n",
    "    def add(self, weights):\n",
    "        self.weights.append(weights)\n",
    "        if len(self.weights) > 50:\n",
    "            self.weights = self.weights[::2]\n",
    "\n",
    "HoF = HallOfFame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "K.clear_session()\n",
    "#model = FullModel(game_state_X.shape, D)\n",
    "\n",
    "def make_agent():\n",
    "    return TensorAgent(game_state_X.shape, D, weights=None, model=FullModel(game_state_X.shape, D))\n",
    "\n",
    "total_step = 600000\n",
    "\n",
    "tensor_agent = make_agent()\n",
    "\n",
    "original = 'tensor_agent'\n",
    "copies = [f'tensor_agent-copy{i}' for i in range(3)]\n",
    "additional = [f'tensor_agent{i}' for i in range(3)]\n",
    "\n",
    "agents = {\n",
    "    original: tensor_agent\n",
    "}\n",
    "\n",
    "for n in copies:\n",
    "    agents[n] = tensor_agent.clone()\n",
    "\n",
    "for n in additional:\n",
    "    agents[n] = make_agent()\n",
    "\n",
    "train = {a: False for a in agents}\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights\n",
      "weights loaded\n"
     ]
    }
   ],
   "source": [
    "tensor_agent.model.load_weights('self-play2-600k.h5')\n",
    "HoF.add(tensor_agent.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_code.tensor_agent.layers import NoisyDense\n",
    "\n",
    "for layer in tensor_agent.model.online.layers + tensor_agent.model.target.layers:\n",
    "    if type(layer) == NoisyDense:\n",
    "        layer.w_sigma.initializer.run(session=K.get_session())\n",
    "        layer.b_sigma.initializer.run(session=K.get_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentInfo(object):\n",
    "    def __init__(self, agents, moving_rewards):\n",
    "        self.agents = agents\n",
    "        self.moving_rewards = moving_rewards\n",
    "\n",
    "    def _repr_pretty_(self, pp, cycle):\n",
    "        text = ''\n",
    "        for n, a in self.agents.items():\n",
    "            text +=  '=====================\\n'\n",
    "            text += f'{n} ({a.model.family}) \\n'\n",
    "            text += f'trained: {a.model.steps} \\n'\n",
    "            text += f'moving reward: {moving_rewards[n]:.2f} \\n'\n",
    "        pp.text(text)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.agents.keys().join(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Episode 1377 Step: 900551/2000000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "=====================\n",
       "tensor_agent (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 1.25 \n",
       "=====================\n",
       "tensor_agent-copy0 (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 2.58 \n",
       "=====================\n",
       "tensor_agent-copy1 (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 1.45 \n",
       "=====================\n",
       "tensor_agent-copy2 (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 3.32 \n",
       "=====================\n",
       "tensor_agent0 (adoring_bomb_lover) \n",
       "trained: 0 \n",
       "moving reward: 1.02 \n",
       "=====================\n",
       "tensor_agent1 (dreamy_bomb_lover) \n",
       "trained: 0 \n",
       "moving reward: 4.49 \n",
       "=====================\n",
       "tensor_agent2 (stoic_crate_advocate) \n",
       "trained: 0 \n",
       "moving reward: 4.00 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8018e6d38d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mcurrent_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wd/agent_code/tensor_agent/agent.py\u001b[0m in \u001b[0;36mreward_update\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 per_weights = weights)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards_update\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/wd/agent_code/tensor_agent/model.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, inputs, actions, rewards, per_weights)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_ph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_ph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mper_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         })\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = display(f'Starting...', display_id='progress')\n",
    "\n",
    "\n",
    "n_steps = 2000000\n",
    "game = None\n",
    "current_agents = {}\n",
    "\n",
    "train = {n: False for n in agents.keys()}\n",
    "for n in [original] + copies:\n",
    "    train[n] = True\n",
    "#train['tensor_agent'] = True\n",
    "\n",
    "moving_rewards = {n: 0 for n in agents.keys()}\n",
    "episode_count = 0\n",
    "\n",
    "info = AgentInfo(agents, moving_rewards)\n",
    "agent_display = display(info, display_id='agent_info')\n",
    "\n",
    "\n",
    "\n",
    "for step in range(total_step, n_steps):\n",
    "    if game is None:\n",
    "        episode_count += 1\n",
    "        \n",
    "        #=== Choose Agents ===\n",
    "        agent_choice = np.random.choice(['solo', 'hof', 'clones'], p=[0.1, 0.3, 0.6])\n",
    "        if agent_choice == 'solo':\n",
    "            current_agents = {n: agents[n] for n in [original]}\n",
    "        if len(HoF.weights) > 2 and agent_choice == 'hof':\n",
    "            current_agents = {n: agents[n] for n in [original] + additional}\n",
    "            for n in additional:\n",
    "                agents[n].model.set_weights(HoF.weights[np.random.choice(len(HoF.weights))])\n",
    "        else:\n",
    "            current_agents = {n: agents[n] for n in [original] + copies}\n",
    "        \n",
    "        \n",
    "        #=== Choose Game ===\n",
    "        choice = np.random.choice(['coins', 'deathmatch', 'full'], p=[0.1,0.,0.9])\n",
    "        if choice == 'coins':\n",
    "            game = Game(*Game.create_arena(current_agents.keys(), crate_density=0.), max_duration=100)\n",
    "        elif choice == 'deathmatch' and agent_choice != 'solo':\n",
    "            game = Game(*Game.create_arena(current_agents.keys(), crate_density=0., coins_per_area=0),\\\n",
    "                        max_duration=np.random.randint(100, 401))\n",
    "        else: # 'full'\n",
    "            game = Game(*Game.create_arena(current_agents.keys(),\\\n",
    "                                           crate_density=np.random.uniform(low=0.5, high=1.0)), \\\n",
    "                       max_duration=np.random.randint(100, 401))\n",
    "        dead_rewards = {}\n",
    "\n",
    "    total_step += 1\n",
    "    actions = {}\n",
    "    Xs = {}\n",
    "    for agent in game.agents:\n",
    "        x, y, name, b, _ = agent\n",
    "        \n",
    "        game_state = game.get_game_state(agent)\n",
    "        Xs[name] = game_state_X.get(game_state)\n",
    "        valid_actions = get_valid_actions(x, y, b, game)\n",
    "        actions[name] = current_agents[name].act(Xs[name], train=train[name], valid_actions=valid_actions)\n",
    "    \n",
    "    actions_as_string = {n: choices[actions[n]] for n in actions.keys()}\n",
    "    rewards, events = game.step(actions_as_string)\n",
    "    \n",
    "    ex_rewards = {}\n",
    "    for name in actions.keys():\n",
    "        moving_rewards[name] = 0.99 * moving_rewards[name] + rewards[name]\n",
    "        ex_rewards[name] = rewards[name] + np.sum([events[name][event] * aux_rewards[event] for event in aux_rewards.keys()])\n",
    "    \n",
    "    #=== Delayed reward on death ===\n",
    "    for name in actions.keys():\n",
    "        if events[name][e.GOT_KILLED]:\n",
    "            dead_rewards[name] = [ex_rewards[name], np.maximum(len(game.agents)-1, 1), Xs[name], actions[name]]\n",
    "            \n",
    "            # if there are no more agents to show how much potential there was left in the game,\n",
    "            # the number of coins left is taken as a negative reward (may overestimate)\n",
    "            if len(game.agents) == 0:\n",
    "                dead_rewards[name][0] -= np.sum(game.coins)\n",
    "    \n",
    "    for _,_,name,_,_ in game.agents:\n",
    "        for dr in dead_rewards.values():\n",
    "            dr[0] -= rewards[name]\n",
    "    #===============================\n",
    "    \n",
    "    for _,_,name,_,_ in game.agents:\n",
    "        reward = ex_rewards[name]\n",
    "        for _,_,opponent,_,_ in game.agents:\n",
    "            if opponent != name:\n",
    "                reward -= rewards[opponent] / (len(game.agents) - 1)\n",
    "        if train[name]:\n",
    "            current_agents[name].reward_update([Xs[name], actions[name], reward])\n",
    "            \n",
    "    \n",
    "    agent_display.update(info)\n",
    "    \n",
    "    if step % 100000 == 0:\n",
    "        tensor_agent.model.save(f'models/self-play3/tensor_agent-model{total_step}.h5')\n",
    "    \n",
    "    if game.terminated:\n",
    "        # Apply delayed reward for death\n",
    "        for name, dr in dead_rewards.items():\n",
    "            if train[name]:\n",
    "                current_agents[name].reward_update([dr[2], dr[3], (dr[0]-np.sum(game.coins))/dr[1]])\n",
    "        \n",
    "        for name, a in current_agents.items():\n",
    "            a.end_of_episode() # alt: save=None\n",
    "        \n",
    "        if episode_count % 100 == 0:\n",
    "            HoF.add(tensor_agent.model.get_weights())\n",
    "        \n",
    "        d.update(f'Episode {episode_count} Step: {step+1}/{n_steps}')\n",
    "        game = None\n",
    "\n",
    "d.update(f'Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tensor_agent': True,\n",
       " 'tensor_agent-copy0': True,\n",
       " 'tensor_agent-copy1': True,\n",
       " 'tensor_agent-copy2': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, a in agents.items():\n",
    "    a.model.save(f'{n}-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step: 195/1000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "=====================\n",
       "tensor_agent (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 0.53 \n",
       "=====================\n",
       "tensor_agent-copy0 (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 1.12 \n",
       "=====================\n",
       "tensor_agent-copy1 (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 0.00 \n",
       "=====================\n",
       "tensor_agent-copy2 (hardcore_coin_advocate) \n",
       "trained: 119813 \n",
       "moving reward: 5.40 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'LEFT'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy1': 'BOMB', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy1': 'DOWN', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy1': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'WAIT', 'tensor_agent-copy2': 'WAIT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'BOMB', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy1': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'DOWN', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy1': 'LEFT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy1': 'LEFT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'BOMB', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'LEFT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'BOMB', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'UP', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'RIGHT', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'BOMB', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'WAIT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'WAIT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy1': 'WAIT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy1': 'WAIT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'WAIT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy0': 'UP', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy0': 'DOWN', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy0': 'BOMB', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'BOMB', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'BOMB'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN', 'tensor_agent-copy2': 'UP'}\n",
      "{'tensor_agent': 'UP', 'tensor_agent-copy2': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'LEFT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'RIGHT'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'BOMB'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'UP'}\n",
      "{'tensor_agent': 'DOWN'}\n",
      "{'tensor_agent': 'UP'}\n"
     ]
    }
   ],
   "source": [
    "d = display(f'Starting...', display_id='progress_test')\n",
    "\n",
    "n_steps = 1000\n",
    "game = None\n",
    "\n",
    "current_agents = {n: agents[n] for n in [original] + copies}\n",
    "train = {n: False for n in agents.keys()}\n",
    "moving_rewards = {n: 0 for n in current_agents.keys()}\n",
    "\n",
    "info = AgentInfo(current_agents, moving_rewards)\n",
    "agent_display = display(info, display_id='agent_info_test')\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    if game is None:\n",
    "        game = Game(*Game.create_arena(current_agents.keys(), crate_density=0.75))\n",
    "\n",
    "    actions = {}\n",
    "    Xs = {}\n",
    "    img = np.zeros((17, 17, 3))\n",
    "    \n",
    "    for agent in game.agents:\n",
    "        x, y, name, b, _ = agent\n",
    "        \n",
    "        game_state = game.get_game_state(agent)\n",
    "        img[:,:,0] = (game_state['arena'] == -1) * 0.75\n",
    "        img[:,:,0] += game_state['arena'] == 1\n",
    "        coins = game_state['coins']\n",
    "        for i in range(len(coins)):\n",
    "            img[coins[i][0], coins[i][1], 1] = 0.75\n",
    "        img[x,y,1] = 1\n",
    "        img[:,:,2] = game_state['explosions'] / np.max(game_state['explosions'])\n",
    "        bombs = game_state['bombs']\n",
    "        \n",
    "        for i in range(len(bombs)):\n",
    "            img[bombs[i][0], bombs[i][1], 2] = 0.75 - bombs[i][2] / (s.bomb_timer) / 2\n",
    "        \n",
    "        Xs[name] = game_state_X.get(game_state)\n",
    "        valid_actions = get_valid_actions(x, y, b, game)\n",
    "        actions[name] = current_agents[name].act(Xs[name], train=train[name], valid_actions=valid_actions)\n",
    "    \n",
    "    imgs.append(img)\n",
    "    \n",
    "    actions_as_string = {n: choices[actions[n]] for n in actions.keys()}\n",
    "    print(actions_as_string)\n",
    "    rewards, events = game.step(actions_as_string)\n",
    "    \n",
    "    for name in actions.keys():\n",
    "        moving_rewards[name] = 0.99 * moving_rewards[name] + rewards[name]\n",
    "    \n",
    "    d.update(f'Step: {step+1}/{n_steps}')\n",
    "    agent_display.update(info)\n",
    "    \n",
    "    if game.terminated:\n",
    "        #for name, a in agents.items():\n",
    "            #a.end_of_episode(save='tensor_agent-model.h5') # alt: save=None\n",
    "        \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animation(imgs, interval=50):\n",
    "    import matplotlib.animation\n",
    "    \n",
    "    steps = len(imgs)\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(t):\n",
    "        plt.imshow(imgs[t])\n",
    "\n",
    "    ani = matplotlib.animation.FuncAnimation(fig, animate, frames=steps, interval=interval)\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = animation(imgs[:40], interval=300)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "! rm anim/*\n",
    "for i in range(len(imgs)):\n",
    "    Image.fromarray(np.uint8(imgs[i]*255)).resize((17*10,17*10)).save('anim/{:0>3d}.png'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "! convert anim/*.png movie.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
