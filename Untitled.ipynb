{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from settings import s, e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer():\n",
    "    \n",
    "    def __init__(self, size = 1000, dimension=3):\n",
    "        self.size = size\n",
    "        self.buffer=[]\n",
    "        self.dimension=dimension\n",
    "        for i in range(dimension):\n",
    "            self.buffer.append([])\n",
    "            \n",
    "    def add(self, *args):\n",
    "        experience = args\n",
    "        for i in range(self.dimension):\n",
    "            if len(self.buffer[i])+len(experience[i])>=self.size:\n",
    "                self.buffer[i]=self.buffer[i][(len(self.buffer[i])+len(experience[i]))-self.size:]\n",
    "            self.buffer[i].extend(experience[i])\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        indexes=range(len(self.buffer[0]))\n",
    "        rand=random.sample(indexes, batch_size)\n",
    "        batch=[]\n",
    "        for i in range(self.dimension):\n",
    "            batch.append([])\n",
    "            batch[i]=np.array(self.buffer[i])[rand]\n",
    "        return batch        \n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer=[]\n",
    "        for i in range(self.dimension):\n",
    "            self.buffer.append([])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delayed_reward(reward, disc_factor):\n",
    "    reward=np.array(reward)\n",
    "    dela_rew=np.empty_like(reward)\n",
    "    storage=0\n",
    "    for i in range(len(reward)):\n",
    "        j=len(reward)-i\n",
    "        dela_rew[j]=storage*disc_factor+reward[j]\n",
    "        storage = storage+reward[j]\n",
    "    return dela_rew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "choices = ['RIGHT', 'LEFT', 'UP', 'DOWN', 'BOMB', 'WAIT']\n",
    "\n",
    "# channels: arena, self, others (3), bombs, explosions, coins -> c = 8 (see get_x)\n",
    "c = 8\n",
    "\n",
    "\n",
    "def setup(agent):\n",
    "    K.clear_session()\n",
    "    \n",
    "    D = len(choices)\n",
    "    \n",
    "    #========================\n",
    "    #  Define Model\n",
    "    #========================\n",
    "    \n",
    "    inputs = Input(shape=(s.cols, s.rows, c))\n",
    "    x = Conv2D(16, 3)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(D, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=pred)\n",
    "    #model.compile(loss=\"hinge\", optimizer=\"adam\")\n",
    "\n",
    "    agent.model = model\n",
    "\n",
    "    \n",
    "    #========================\n",
    "    #  Define Training Update\n",
    "    #========================\n",
    "\n",
    "    action_holder = Input(shape=(1,), dtype='int32')  # in j=0,...,D-1\n",
    "    reward_holder = Input(shape=(1,))   ##target Q???\n",
    "    \n",
    "    \n",
    "    \n",
    "    # applies a mask to the outputs so that only the prediction for the chosen action is considered\n",
    "    responsible_weight = tf.reduce_sum(tf.boolean_mask(pred, tf.one_hot(action_holder, D)[:,0,:])) ###Qvalue\n",
    "\n",
    "    loss = - (tf.log(responsible_weight) * reward_holder)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.1)\n",
    "    update = optimizer.minimize(loss)\n",
    "    \n",
    "    \n",
    "    # Initialize all variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    K.get_session().run(init_op)\n",
    "\n",
    "    # the alternative Keras way:\n",
    "    #training_model = Model(inputs=[inputs, action_holder, reward_holder], outputs=loss)\n",
    "    #training_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer='Adam')\n",
    "\n",
    "    \n",
    "    agent.update = update\n",
    "    \n",
    "    agent.inputs = inputs\n",
    "    agent.action_holder = action_holder\n",
    "    agent.reward_holder = reward_holder\n",
    "    \n",
    "    #agent.Xs_episode = replay_buffer() \n",
    "    #agent.actions_episode = replay_buffer()\n",
    "    #agent.rewards_episode = replay_buffer()\n",
    "    agent.buffer = replay_buffer() #total buffer\n",
    "    agent.episode_buffer = replay_buffer() #episode buffer\n",
    "    #agent.Xs = replay_buffer()\n",
    "    #agent.actions=replay_buffer()\n",
    "    #agent.rewards= replay_buffer()\n",
    "    agent.epsilon=0.1\n",
    "\n",
    "    np.random.seed()\n",
    "\n",
    "def act(agent):\n",
    "    # agent.game_state\n",
    "    print('Epsilon greedy')\n",
    "    X = get_x(agent.game_state)\n",
    "    agent.X = X\n",
    "\n",
    "    #agent.next_action = np.random.choice(choices, p=[.23, .23, .23, .23, .08, .00])\n",
    "    if np.random.rand(1) > agent.epsilon:\n",
    "        pred = agent.model.predict(np.array([X]))\n",
    "        agent.action_choice = np.argmax(pred)\n",
    "        agent.next_action = choices[agent.action_choice]\n",
    "    else:\n",
    "        agent.next_action = np.random.choice(choices, p=[.23, .23, .23, .23, .08, .00])\n",
    "    print(\"================================\")\n",
    "    print(agent.next_action)\n",
    "\n",
    "def reward_update(agent):\n",
    "    print('Update')\n",
    "    events = agent.events\n",
    "    reward = 0\n",
    "    reward += events.count(e.COIN_FOUND)\n",
    "    reward += events.count(e.COIN_COLLECTED)\n",
    "    reward += 2 * events.count(e.KILLED_OPPONENT)\n",
    "    reward -= 10 * events.count(e.KILLED_SELF)\n",
    "    reward -= 5 * events.count(e.GOT_KILLED)\n",
    "    reward += 20 * events.count(e.SURVIVED_ROUND)\n",
    "    agent.reward = reward\n",
    "    agent.episode_buffer.add(agent.X, agent.action_choice, agent.reward)\n",
    "    agent.Xs.append([agent.X])\n",
    "    agent.actions.append([agent.action_choice])\n",
    "    agent.rewards.append([agent.reward])\n",
    "\n",
    "def end_of_episode(agent):\n",
    "    #model = agent.model\n",
    "    #model.train_on_batch(x, y, class_weight=None)\n",
    "    print(agent.episode_buffer.buffer[:,2])\n",
    "    #agent.episode_buffer.buffer[:,2]=delayed_reward(agent.episode_buffer.buffer[:,2],disc_factor)##delayed rewards\n",
    "    x, action, reward = agent.episode_buffer.buffer\n",
    "    agent.buffer.add(x, action, delayed_reward(reward))\n",
    "    agent.episode_buffer.clear() #clear episode_buffer\n",
    "    #batch=agent.buffer.sample(10)#get batch to train on random experiences\n",
    "    #agent.rewards_buffer.add(delayed_reward(agent.rewards_episode.buffer, disc_factor)) #add delayed rewards\n",
    "    #agent.Xs_buffer.add(agent.Xs_episode.buffer)\n",
    "    #agent.actions_buffer.add(agent.actions.buffer)\n",
    "    agent.Xs, agent.actions, agent.rewards = agent.buffer.sample(10)  #get batch to train on random experiences\n",
    "    #agent.actions=batch[:,1]\n",
    "    #agent.rewards=batch[:,2]\n",
    "    sess = K.get_session()\n",
    "    sess.run([agent.update], feed_dict={agent.inputs: np.array(agent.Xs), agent.reward_holder:np.array(agent.rewards),agent.action_holder:np.array(agent.actions)})\n",
    "    print('End of Episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-d1b4c789df83>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d1b4c789df83>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def build_network():\n",
    "    inputs = Input(shape=(s.cols, s.rows, c))\n",
    "    x = Conv2D(16, 3)(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    pred = Dense(D, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=pred)\n",
    "\n",
    "    return model, pred\n",
    "    \n",
    "#create networks\n",
    "q_net, q_pred = build_network()\n",
    "t_net, t_pred = build_network()\n",
    "\n",
    "action_holder = Input(shape=(1,), dtype='int32')  # in j=0,...,D-1\n",
    "reward_holder = Input(shape=(1,))   ##target Q???\n",
    "q_target_holder = Input(shape=(1,))\n",
    "\n",
    "# applies a mask to the outputs so that only the prediction for the chosen action is considered\n",
    "output = q_pred+ tf.subtract(t_pred, tf.reduce_mean(t_pred, axis=1, keepdims=True))\n",
    "responsible_weight_q = tf.reduce_sum(tf.boolean_mask(output, tf.one_hot(action_holder, D)[:,0,:])) ###Qvalue\n",
    "\n",
    "loss = - (tf.log(responsible_weight) * reward_holder)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "update = optimizer.minimize(loss)\n",
    "    \n",
    "    \n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "K.get_session().run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Episode\n"
     ]
    }
   ],
   "source": [
    "end_of_episode(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'step' The number of steps in the episode so far, starting at 1.\n",
    "\n",
    "'arena' A 2D numpy array describing the tiles of the game board. Its entries are 1 for\n",
    "crates, −1 for stone walls and 0 for free tiles.\n",
    "\n",
    "'self' A tuple (x, y, n, b) describing your own agent. x and y are its coordinates on\n",
    "the board, n its name and b ∈ {0, 1} a \u001d",
    "ag indicating if the 'BOMB' action is\n",
    "possible (i.e. no own bomb is currently ticking).\n",
    "\n",
    "'others' A list of tuples like the one above for all opponents that are still in the game.\n",
    "\n",
    "'bombs' A list of tuples (x, y, t) of coordinates and countdowns for all active bombs.\n",
    "\n",
    "'explosions' A 2D numpy array stating, for each tile, for how many steps an explosion will\n",
    "be present. Where there is no explosion, the value is 0.\n",
    "\n",
    "'coins' A list of coordinates (x, y) for all currently collectable coins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(game_state):\n",
    "    arena = game_state['arena']\n",
    "    self = game_state['self']\n",
    "    others = game_state['others']\n",
    "    bombs = game_state['bombs']\n",
    "    explosions = game_state['explosions']\n",
    "    coins = game_state['coins']\n",
    "    # channels: arena, self, others (3), bombs, explosions, coins -> c = 8\n",
    "    c = 8\n",
    "    X = np.zeros((s.cols, s.rows, c))\n",
    "    \n",
    "    X[:,:,0] = arena\n",
    "    \n",
    "    X[self[0],self[1],1] = self[3]\n",
    "    \n",
    "    for i in range(len(others)):\n",
    "        X[others[i][0], others[i][1], i+2] = others[i][3]\n",
    "    \n",
    "    for i in range(len(bombs)):\n",
    "        X[bombs[i][0], bombs[i][1], 5] = bombs[i][2]\n",
    "    \n",
    "    X[:,:,6] = explosions\n",
    "    \n",
    "    for i in range(len(coins)):\n",
    "        X[coins[i][0], coins[i][1], 7] = 1\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_buffer():\n",
    "    def __init__(self, size = 5):\n",
    "        self.size = size\n",
    "        self.buffer = []\n",
    "    def add(self, experience):\n",
    "        if len(self.buffer)+len(experience)>=self.size:\n",
    "            self.buffer=self.buffer[(len(self.buffer)+len(experience))-self.size:]\n",
    "        self.buffer.extend(experience)\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4, 5, 6]) 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "rlist=np.array([np.array([1,2,3]),np.array([4,5,6]),1,2,3,4,5])\n",
    "\n",
    "print(np.array(rlist)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79653437])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist=[1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delayed_reward(reward, disc_factor):\n",
    "    \"\"\" function that calculates delayed rewards for given list of rewards and discount_factor.\"\"\"\n",
    "    reward_array=np.array(reward)\n",
    "    dela_rew=np.empty_like(reward)\n",
    "    storage=0\n",
    "    for i in range(len(reward)):\n",
    "        print('1')\n",
    "        j=len(reward)-i-1\n",
    "        print('2')\n",
    "        dela_rew[j]=storage*disc_factor+reward[j]\n",
    "        print('3')\n",
    "        storage = storage+reward[j]\n",
    "    print(\"end\")\n",
    "    return dela_rew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] [1 2] [array([7, 5, 2]), array([7, 5, 2])]\n"
     ]
    }
   ],
   "source": [
    "buffer=replay_buffer(dimension=2)\n",
    "X=[np.array([1,2]),np.array([3,2]),np.array([4,3]),([5,4]),5,6,7]\n",
    "y = [1,2,3,4,5,6,7]\n",
    "buffer.add(y,y)\n",
    "a,b=buffer.sample(2)\n",
    "print(a,b, buffer.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "    def __init__(self, capacity): #initalize tree and data with only zeros\n",
    "        \n",
    "        self.capacity=capacity\n",
    "        self.tree=np.zeros(2*capacity-1)#actual tree\n",
    "        self.data=np.zeros(capacity, dtype=object)#here the data is stored\n",
    "        self.pointer=0\n",
    "        self.total_priority=0   #for normalization get priority of root\n",
    "\n",
    "        \n",
    "    def update(self, index, priority):\n",
    "        old_priority=self.tree[index]\n",
    "        self.tree[index]=priority\n",
    "        \n",
    "        #update upper parent nodes\n",
    "        while index!=0: #as long as root is not reached\n",
    "            \n",
    "            index=int(np.floor((index-1)/2))\n",
    "            self.tree[index]= self.tree[index]+priority-old_priority\n",
    "            \n",
    "    def add(self, priority, new_data):\n",
    "        \n",
    "        self.data[self.pointer]=new_data\n",
    "        t_index=self.pointer+self.capacity-1\n",
    "        self.update(t_index, priority)\n",
    "            \n",
    "        self.pointer+=1\n",
    "        if self.pointer>=self.capacity:\n",
    "            self.pointer=0\n",
    "        \n",
    "        self.total_priority=self.tree[0]\n",
    "        \n",
    "    def get_leave(self, value):\n",
    "        bottom=False\n",
    "        parent=0\n",
    "        \n",
    "        while not bottom:\n",
    "            \n",
    "            left_child=parent*2+1\n",
    "            right_child=left_child+1\n",
    "            if left_child>=len(self.tree):\n",
    "                break\n",
    "            \n",
    "            if self.tree[left_child]>=value:\n",
    "                parent=left_child\n",
    "            else:\n",
    "                value-=left_child\n",
    "                parent=right_child\n",
    "        leave_index=parent\n",
    "        return leave_index, self.tree[leave_index], self.data[leave_index-self.capacity+1]\n",
    "            \n",
    "\n",
    "            \n",
    "class PER_buffer(object):\n",
    "    \n",
    "    def __init__(self, capacity, PER_a, PER_b, PER_e, anneal):\n",
    "    \n",
    "        self.capacity=capacity   \n",
    "        self.tree=SumTree(capacity)\n",
    "        self.default_max_p=1\n",
    "        self.PER_a=PER_a  #Hyperparameter to introduce randomness in PER from 0to1\n",
    "        self.PER_b=PER_b  #Hyperparameter to scale influence of weight needs to be annealed during lerning\n",
    "        self.anneal=anneal #annealing factor for PER_b\n",
    "        self.PER_e=PER_e      #constant to insure that priority never gets zero\n",
    "        \n",
    "    def add(self, *args):\n",
    "        experience = args\n",
    "        #new experience gets maximum priority\n",
    "        max_p=np.max(self.tree.tree[-self.tree.capacity:]) #search for maximal priority in leave nodes\n",
    "        \n",
    "        if max_p==0:                        #priority can't be zero because then experience would never be picked\n",
    "            max_p=self.default_max_p\n",
    "            \n",
    "        self.tree.add(max_p, experience)\n",
    "    def sample(self, k):  #k:how many experiences in one sample\n",
    "        \n",
    "        priority_range = self.tree.total_priority/k\n",
    "        minibatch=[]\n",
    "        weights=[]\n",
    "        \n",
    "        #to normalize the weights, the maximal weight needs to be calculated\n",
    "        max_weight=1/(k*np.min(self.tree.tree[-self.tree.capacity:]))**self.PER_b\n",
    "        \n",
    "        for i in range(k):\n",
    "            lower_bound = priority_range*i\n",
    "            upper_bound=lower_bound+priority_range\n",
    "            \n",
    "            #now get a random sample from that range\n",
    "            value=np.random.uniform(lower_bound,upper_bound)\n",
    "            leave_index, value_priority, value_data = self.tree.get_leave(value)\n",
    "            \n",
    "            prob_weight=value_priority/self.tree.total_priority\n",
    "            weight=(1/(k*prob_weight)**self.PER_b)/max_weight\n",
    "            \n",
    "            weights.append(prob_weight)\n",
    "            minibatch.append(value_data)\n",
    "        self.PER_b=np.minimum(1., self.PER_b+self.anneal)\n",
    "        return minibatch, weights\n",
    "    \n",
    "    def update(self, idxs, errors):\n",
    "        ''' It is important to use data idx here, not tree '''\n",
    "        \n",
    "        priorities=np.abs(errors)+self.PER_e\n",
    "        priorities=np.minimum(priorities, self.default_max_p)\n",
    "        pri_a=priorities**self.PER_a   #modified priority that is actually used\n",
    "        idxs+=self.capacity-1\n",
    "        for i, p in zip(idxs, pri_a):\n",
    "            self.tree.update(i, p)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0) (1, 1, 1) (7, 8, 9) (10, 11, 12) (13, 14, 15)] [0.31622777 0.4472136  0.54772256 0.63245553 1.        ]\n"
     ]
    }
   ],
   "source": [
    "a=PER_buffer(5, 0.5, 0.1, 0.1, 0.1)\n",
    "a.add(1,2,3)\n",
    "a.add(4,5,6)\n",
    "a.add(7,8,9)\n",
    "a.add(10,11,12)\n",
    "a.add(13,14,15)\n",
    "a.add(0,0,0)\n",
    "a.add(1,1,1)\n",
    "idx=np.arange(5)\n",
    "errors=np.array([0,0.1,0.2,0.3,0.99])\n",
    "a.update(idx, errors)\n",
    "print(a.tree.data, a.tree.tree[-len(a.tree.data):])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "capacity=15\n",
    "print(np.log2(capacity+1)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.minimum([1,2,3,4],3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.minimum(1.,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
