%!TEX root = ./bomberchamp.tex
\subsection{Q-learning}
In Q-learning \cite{DBLP:journals/corr/HasseltGS15} instead of the state-values, Q-values are being used. They are defined as the current reward depending on the chosen action and the discounted future rewards under the premise of always using the policy $\pi$ for further decision making after the initial action choice:
\begin{equation}
Q_\pi(s,a)=\mathbb{E}\left[R\right]|_{a,s,\pi}.
\end{equation} 
The best policy is then determined by maximizing $Q_\pi$ over the actions in every step.
%TODO why is this better than the state value function??
Usually it is not possible to learn every action for every state explicitly because the problems are often to extensive. Therefore the Q-values get approximated by a parametrized function $Q(s,a,\theta_t)$ with $\theta_t$ being the parameters. The Q-function is then updated towards the target value
\begin{equation}\label{eq:Q-target}Y_{t}^{\mathrm{Q}} \equiv R_{t+1}+\gamma \max _{a} Q\left(s_{t+1}, a , \theta_{t}\right)
\end{equation}
after each action via:
\begin{equation}
\theta_{t+1}=\theta_{t}+\alpha\left(Y_{t}^{\mathrm{Q}}-Q\left(s_{t}, a; \theta_{t}\right)\right) \nabla_{\theta_{t}} Q\left(s_{t}, a ; \theta_{t}\right).
\end{equation}
This makes it an off-policy algorithm, as the optimal $Q$-value: $Q^*$ is approximated by $Q$ directly (regardless of the followed policy) in contrast to for example SARSA. The policy is still important as it determines, which state-action pairs are used to update the model \cite{Sutton:1998:IRL:551283}. 
Q-learning is also a model free approach, as it doesn't make a model of the environment but instead directly estimates $Q^*$.\\
%In our case with bomberman, the problem is fully observable, meaning that the agent knows the entire state of the environment at every step of the game.
\subsection{Q-Networks}
For some reinforcement learning problems, the simplest implementation of the Q-learning algorithm, a table is perfectly sufficient to find a good policy.
If we consider more complicated problems with bigger inputs other methods are needed. For state of the art reinforcement learning, usually neural networks are being used.
They are especially convenient as they are trained from raw inputs, which makes handcrafted features redundant \cite{DBLP:journals/corr/MnihKSGAWR13}.
Neural Networks which are implemented to learn with the Q-algorithm are called Q-networks.
They are non linear function approximations for $Q(s,a,\theta_t)$.
To update the Q-network the loss function
\begin{equation}
L_{t}\left(\theta_{t}\right)=\mathbb{E}_{s, a \sim \rho(s,a)}\left[\left(Y_{t}^{\mathrm{Q}}-Q\left(s, a ; \theta_{t}\right)\right)^{2}\right]
\end{equation}
%TODO Erwartungswert gro√üschreiben
is used, where $\rho(s, a)$ is a probability distribution over states and actions. To make this compatible with the Q-learning algorithm, the weights need to be updated at every step and the expectations exchanged with samples from the probability distribution $\rho(s,a)$.
\subsection{DQN}
Deep Q-networks \cite{DBLP:journals/corr/MnihKSGAWR13} are multilayered neural networks which make use of the Q-learning algorithm. The two mayor improvements of the model are the use of two separate networks and an experience buffer.
The two networks are called online network and target network. The target network is used to calculate the targets $Y_{t}^{\mathrm{DQN}}$. It has the same structure as the online network but to make learning more stable, the weights of the target network $\theta_t^-$ stay constant for a longer time. 
They are being copied from the online network every $\tau$ steps.
The target is then calculated by:
\begin{equation}
Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(s_{t+1}, a ; \theta_{t}^{-}\right).
\end{equation}
If only one network was used, an update of $Q(s,a)$ would often not only lead to a higher value of $Q(s_t,a_t)$ but also to higher expected Q-values $Q(s_{t+1},a)$ for all actions. If the target was also calculated by this network this could then lead to oscillations or divergence of the policy \cite{DBLP:journals/corr/MnihKSGAWR13}.
The additional improvement of DQN is experience replay. Without experience replay, only new experiences are used in training and discarded right afterwards.
Therefore important but rare experiences are almost immediately forgotten and the updates are not independent and identical distributed but strongly correlated. %TODO why is this bad?
To address this problem, an experience buffer is implemented. There the experiences are stored and then at training time sampled uniformly at random. Usually a simple FIFO algorithm is being used. But there are more sophisticated methods for this, one of those is discussed later.