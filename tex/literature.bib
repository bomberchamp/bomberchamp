@inproceedings{Hessel2018RainbowCI,
  title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
  author={Matteo Hessel and Joseph Modayil and Hado P. van Hasselt and Tom Schaul and Georg Ostrovski and Will Dabney and Dan Horgan and Bilal Piot and Mohammad Gheshlaghi Azar and David Silver},
  booktitle={AAAI},
  year={2018}
}

@article{DBLP:journals/corr/SchaulQAS15,
	  author    = {Tom Schaul and
		                 John Quan and
					                Ioannis Antonoglou and
							               David Silver},
	    title     = {Prioritized Experience Replay},
	      journal   = {CoRR},
	        volume    = {abs/1511.05952},
		  year      = {2015},
		    url       = {http://arxiv.org/abs/1511.05952},
		      archivePrefix = {arXiv},
		        eprint    = {1511.05952},
			  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
			    biburl    = {https://dblp.org/rec/bib/journals/corr/SchaulQAS15},
			      bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/HasseltGS15,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-learning},
  journal   = {CoRR},
  volume    = {abs/1509.06461},
  year      = {2015},
  url       = {http://arxiv.org/abs/1509.06461},
  archivePrefix = {arXiv},
  eprint    = {1509.06461},
  timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HasseltGS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/WangFL15,
  author    = {Ziyu Wang and
               Nando de Freitas and
               Marc Lanctot},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06581},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06581},
  archivePrefix = {arXiv},
  eprint    = {1511.06581},
  timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WangFL15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/FortunatoAPMOGM17,
  author    = {Meire Fortunato and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Jacob Menick and
               Ian Osband and
               Alex Graves and
               Vlad Mnih and
               R{\'{e}}mi Munos and
               Demis Hassabis and
               Olivier Pietquin and
               Charles Blundell and
               Shane Legg},
  title     = {Noisy Networks for Exploration},
  journal   = {CoRR},
  volume    = {abs/1706.10295},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.10295},
  archivePrefix = {arXiv},
  eprint    = {1706.10295},
  timestamp = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/FortunatoAPMOGM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Sutton1988,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
issn="1573-0565",
doi="10.1007/BF00115009",
url="https://doi.org/10.1007/BF00115009"
}

@Article{Peng1996,
author="Peng, Jing
and Williams, Ronald J.",
title="Incremental multi-step Q-learning",
journal="Machine Learning",
year="1996",
month="Mar",
day="01",
volume="22",
number="1",
pages="283--290",
abstract="This paper presents a novel incremental algorithm that combines Q-learning, a well-known dynamic-programming based reinforcement learning method, with the TD($\lambda$) return estimation process, which is typically used in actor-critic learning, another well-known dynamic-programming based reinforcement learning method. The parameter $\lambda$ is used to distribute credit throughout sequences of actions, leading to faster learning and also helping to alleviate the non-Markovian effect of coarse state-space quatization. The resulting algorithm.Q($\lambda$)-learning, thus combines some of the best features of the Q-learning and actor-critic learning paradigms. The behavior of this algorithm has been demonstrated through computer simulations.",
issn="1573-0565",
doi="10.1007/BF00114731",
url="https://doi.org/10.1007/BF00114731"
}

@misc{ OpenAI_dota,
      author = {OpenAI},
      title = {OpenAI Five},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}}

