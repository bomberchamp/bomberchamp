\documentclass[12pt]{article}
%\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{caption}
\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\usepackage{amssymb}
\bibliography{literature.bib}
\title{Machine Learning: Final project bomberchamp}
\date{today}
\begin{document}





\section{Introduction}
% Introduction
% we implemented a rainbow dqn

\subsection{Reinforcement Learning}
\input{rlintro}
% Reinforcement Learning basics a
\subsection{Neural Networks}
% basics j

\section{DQN}
\input{DQN}
% a

\section{Rainbow / DQN Extensions}
\input{DQN_extensions}
\subsection{Multi-Step Learning} % j
% 

\section{Other things}
\subsection{Augmented data}
\subsection{Centring of agent} % j / a
\subsection{Invalid actions} % 
\subsection{Auxilliary Reward Design}
\subsection{Minigame Collection} % j
\subsection{Self-Play (maybe)}

\section{Observation}
\subsection{It does not work}
% nothing works
% solution: make it better
\subsection{Network scaling} % j
% different networks from minigame to full game
% lots of graphs

% our learning process


\section{Summary and Improvements}
% improvements: distributional DQN, train for 3 months

% improvement of game setup: j
% - make provided framework easy to use
% -- currently main.py, settings.py and possibly callbacks.py has to be changed for a simple switch between train and test mode
% -- separate rendering and environment, so that the environment can be called from e.g. a jupyter notebook
% -- provide main.py or python notebook 





\section{Model}



\section{Training process}

\subsection{Data augmentation}
As the inputs for our bomberchamp agent are symmetric, we used data augmentation to increase the number of samples for training and to make learning more symmetric.
From each original sample, three augmented samples were created. The augmented samples consist of change of left and right, change of up and down, both combined.
\subsection{Centring of the agent}
To simplify learning for our agent, it was centred on the board, so that it stays at a fixed position while the environment (coins, crates, other agents) move around it. This is very common in the Atari games on which most implementations of DQN agents are tested, including the Rainbow agent without distributional reinforcement learning from which we took most of our initial hyper parameters. Therefore we thought, a more similar setting would be beneficial to our agent. To implement this, the size of the board was increased to four times its original size and the agent placed in the middle of the new board.
\subsection{Self play}
%TODO how was it implemented etc
For training purposes a simple agent implementation that follows the rules of bomberman and plays reasonably well was provided.
To exploit this we wanted to first train our agent by classifying inputs generated with the simple agents. %TODO why didn't this work?
This is why we implemented a self play strategy. This was also useful, as we could then use google colab and train more than one agent at the same time while using the same neural network.



\printbibliography

\end{document}
