\documentclass[12pt]{article}
%\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{caption}
\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\usepackage{amssymb}
\bibliography{literature.bib}
\title{Machine Learning: Final project bomberchamp}
\date{today}
\begin{document}





\section{Introduction}
% Introduction
% we implemented a rainbow dqn

\subsection{Reinforcement Learning}
\input{rlintro}
% Reinforcement Learning basics a
\subsection{Neural Networks}
% basics j

\section{DQN}
% a

\section{Rainbow / DQN Extensions}
\input{DQN_extensions}
\subsection{Multi-Step Learning} % j
% 

\section{Other things}
\subsection{Augmented data}
\subsection{Centring of agent} % j / a
\subsection{Invalid actions} % 
\subsection{Auxilliary Reward Design}
\subsection{Minigame Collection} % j
\subsection{Self-Play (maybe)}

\section{Observation}
\subsection{It does not work}
% nothing works
% solution: make it better
\subsection{Network scaling} % j
% different networks from minigame to full game
% lots of graphs

% our learning process


\section{Summary and Improvements}
% improvements: distributional DQN, train for 3 months

% improvement of game setup: j
% - make provided framework easy to use
% -- currently main.py, settings.py and possibly callbacks.py has to be changed for a simple switch between train and test mode
% -- separate rendering and environment, so that the environment can be called from e.g. a jupyter notebook
% -- provide main.py or python notebook 





\section{Model}


\subsection{Q-learning}
The true value of an action under a chosen policy is defined as the sum of the current reward for that action and the discounted future rewards:
\begin{equation}
Q_\pi(s,a)= E\left[\sum_{n=1}^{\infty} \gamma^{n-1}R_n^n\right] \Bigg|_{a,s,\pi}.
\end{equation} 
$\gamma\,\epsilon\,[0,1]$ denotes the discounting factor.
The best policy is then determined by maximizing $Q_\pi$ over the actions in every step.
Usually it is not possible to learn every action for every state explicitly because the problems are often to extensive. Therefore The Q-values get approximated by a parametrized function $Q(a,s,\theta_t)$. Which is updated towards the target value $Y_{t}^{\mathrm{Q}}$:
\begin{equation}
Y_{t}^{\mathrm{Q}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right)
\end{equation}
after each action via:
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha\left(Y_{t}^{\mathrm{Q}}-Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right)\right) \nabla_{\boldsymbol{\theta}_{t}} Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right).
\end{equation}
\cite{DBLP:journals/corr/HasseltGS15}
This makes it an off-policy algorithm, as the optimal $Q$-value: $Q^*$ is approximated by $Q$ directly (regardless of the followed policy) in contrast to for example SARSA. The policy is still important as it determines, which state-action pairs are used to update the model\cite{Sutton:1998:IRL:551283}. 
Q-learning is also a model free approach, as it doesn't a model of the environment but instead directly estimates $Q^*$.
%In our case with bomberman, the problem is fully observable, meaning that the agent knows the entire state of the environment at every step of the game.

\subsection{Neural Networks}
For some reinforcement learning problems, the simplest implementation of the Q-learning algorithm, a table is perfectly sufficient to find a good policy.
If we consider more complicated problems with bigger inputs other methods are needed. For state of the art reinforcement learning, usually neural networks are being used. %TODO explain neural networks
They are especially convenient as they are trained from raw inputs, which makes handcrafted features redundant \cite{DBLP:journals/corr/MnihKSGAWR13}.
Neural Networks which are implemented to learn with the Q-algorithm are called Q-networks.
They are a non linear function approximation for $Q(s,a,\theta_t)$.
To update the Q-network the loss function
\begin{equation}
L_{t}\left(\theta_{t}\right)=\mathbb{E}_{s, a \sim \rho(s,a)}\left[\left(Y_{t}^{\mathrm{Q}}-Q\left(s, a ; \theta_{t}\right)\right)^{2}\right]
\end{equation}
where $\rho(s, a)$ is a probability distribution over states and actions. To make this compatible with the Q-learning algorithm, the weights need to be updated at every step and the expectations exchanged with samples from the probability distribution $\rho(s,a)$.

\subsection{DQN}
To get from Q-networks to Deep Q-networks (DQN) there are two mayor improvements.
First: two multi-layered neural networks are used: The online network and the target network. The target network is used to calculate the targets. They both have the same structure but to make learning more stable, the weights of the target network $\theta_t^-$ stay constant for a longer time. 
They are being copied from the online network every $\tau$ steps.
The target is then calculated by:
\begin{equation}
Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \theta_{t}^{-}\right).
\end{equation}
If everything was made by only one network, an update of $Q(s,a)$ does often not only lead to a higher value of $Q(s_t,a_t)$ but also higher expected Q-values $Q(s_{t+1},a)$ for all actions. If the target is also calculated by this network this can lead to oscillations or divergence of the policy \cite{DBLP:journals/corr/MnihKSGAWR13}.
Additionally DQN introduces experience replay. Without experience replay, only new experiences are used in training and discarded right afterwards.
Therefore important but rare experiences are almost immediately forgotten and the updates are not independent and identical distributed but strongly correlated.
To address this problem, an experience buffer is implemented, in which the experiences are stored and then at training time sampled uniformly at random. Usually a simple FIFO algorithm is being used. But there are more sophisticated methods for this, one of those is discussed later.
\section{Training process}

\subsection{Data augmentation}
As the inputs for our bomberchamp agent are symmetric, we used data augmentation to increase the number of samples for training and to make learning more symmetric.
From each original sample, three augmented samples were created. The augmented samples consist of change of left and right, change of up and down, both combined.
\subsection{Centring of the agent}
To simplify learning for our agent, it was centred on the board, so that it stays at a fixed position while the environment (coins, crates, other agents) move around it. This is very common in the Atari games on which most implementations of DQN agents are tested, including the Rainbow agent without distributional reinforcement learning from which we took most of our initial hyper parameters. Therefore we thought, a more similar setting would be beneficial to our agent. To implement this, the size of the board was increased to four times its original size and the agent placed in the middle of the new board.
\subsection{Self play}
%TODO how was it implemented etc
For training purposes a simple agent implementation that follows the rules of bomberman and plays reasonably well was provided.
To exploit this we wanted to first train our agent by classifying inputs generated with the simple agents. %TODO why didn't this work?
This is why we implemented a self play strategy. This was also useful, as we could then use google colab and train more than one agent at the same time while using the same neural network.



\printbibliography

\end{document}
