\documentclass[12pt]{article}
%\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage{svg}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{caption}
\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{subcaption}
\newsavebox{\largestimage}

\bibliography{literature.bib}
\title{Machine Learning: Final project bomberchamp}
\date{today}
\begin{document}





\section{Introduction}
% Introduction
\input{sections/intro}
% we implemented a rainbow dqn

\subsection{Reinforcement Learning}
\input{rlintro}
% Reinforcement Learning basics a
\subsection{Neural Networks}
\input{sections/neural_net}
% basics j

\section{DQN}
\input{DQN}
% a

\section{Rainbow / DQN Extensions}
\input{DQN_extensions}
\subsection{Multi-Step Learning} % j
% 
\input{sections/multi-step}

\section{Training}
\subsection{Feature space}
\input{sections/feature_space}
\subsection{Augmented data}   %a
\input{augmented}
% inside general seciton about input space \subsection{Centring of agent} % j / a
\subsection{Invalid actions} % 
\input{sections/invalid-actions}
\subsection{Auxilliary Reward Design}
\input{sections/aux-rewards}
\subsection{Minigames} % j
\input{sections/minigame}
\subsection{Network Architecture}
\input{sections/network-arch}
\subsection{Self-Play} % j
\input{sections/self-play}


\section{Observation}
\subsection{It does not work}
% nothing works
% solution: make it better
\subsection{Network scaling} % j
% different networks from minigame to full game
% lots of graphs

\begin{figure}
  \centering
  \includesvg{images/network-arch}
  \caption{Network architecture}
\end{figure}
% our learning process


\section{Summary and Improvements}
% improvements: distributional DQN, train for 3 months
% steps/own bomb in feature space
% RNN (or other things able to capture hidden state), hidden states: time until next bomb of agents, who owns which bomb

% improvement of game setup: j
% - make provided framework easy to use
% -- currently main.py, settings.py and possibly callbacks.py has to be changed for a simple switch between train and test mode
% -- separate rendering and environment, so that the environment can be called from e.g. a jupyter notebook
% -- provide main.py or python notebook 





\section{Model}



\section{Training process}

\subsection{Self play}
%TODO how was it implemented etc
For training purposes a simple agent implementation that follows the rules of bomberman and plays reasonably well was provided.
To exploit this we wanted to first train our agent by classifying inputs generated with the simple agents. %TODO why didn't this work?
This is why we implemented a self play strategy. This was also useful, as we could then use google colab and train more than one agent at the same time while using the same neural network.



\printbibliography

\end{document}
